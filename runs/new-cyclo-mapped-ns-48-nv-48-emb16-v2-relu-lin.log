wandb: Currently logged in as: briling (equireact). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.0
wandb: Run data is saved locally in /home/briling/NequiReact/wandb/run-20230518_200755-3gfvjwp4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run revived-thunder-55
wandb:  View project at https://wandb.ai/equireact/nequireact
wandb:  View run at https://wandb.ai/equireact/nequireact/runs/3gfvjwp4
wandb name new-cyclo-mapped-ns-48-nv-48-emb16-v2-relu-lin
input args Namespace(experiment_name='new-cyclo-mapped', num_epochs=3000, checkpoint=None, device='cuda', subset=None, wandb_name='new-cyclo-mapped-ns-48-nv-48-emb16-v2-relu-lin', logdir='logs', process=False, verbose=False, radius=5.0, max_neighbors=20, sum_mode='node', n_s=48, n_v=48, n_conv_layers=2, distance_emb_dim=16, graph_mode='energy', dropout_p=0.1, dataset='cyclo', random_baseline=False, combine_mode='diff', atom_mapping=True)
Running on device cuda:0
Loading data into memory...
Coords and graphs successfully read from data/cyclo/processed//
Data stdev tensor(9.7629, dtype=torch.float64)
total / train / test / val: 5203 3902 650 651
r0graph=Data(x=[6, 16], edge_index=[2, 30], edge_attr=[30], y=-1.3594611934014402, pos=[6, 3])
input_node_feats_dim=16
input_edge_feats_dim=1
trainable params in model:  2740325
Log directory: logs/new-cyclo-mapped
checkpoint: None
num epochs: 3000
eval_per_epochs: 0
patience: 150
minimum_epochs: 0
models_to_save: []
clip_grad: 100
log_iterations: 100
lr: 0.0001
weight decay: 0.0001
lr scheduler: <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
factor: 0.6
min_lr: 8e-06
mode: max
lr_scheduler_patience: 60
lr_verbose: True
In trainer, metrics is {'mae': MAE()} and std is 9.762939402167014
[Epoch 1; Iter   100/  488] train: loss: 1.0378457
[Epoch 1; Iter   200/  488] train: loss: 1.1171505
[Epoch 1; Iter   300/  488] train: loss: 0.3910922
[Epoch 1; Iter   400/  488] train: loss: 0.6717378
[Epoch 1] mae: 7.641350 val loss: 0.954698
Epochs with no improvement: [ 0 ] and the best   mae  was in  1
[Epoch 2; Iter    12/  488] train: loss: 1.4065034
[Epoch 2; Iter   112/  488] train: loss: 0.3051938
[Epoch 2; Iter   212/  488] train: loss: 0.6067101
[Epoch 2; Iter   312/  488] train: loss: 0.6393332
[Epoch 2; Iter   412/  488] train: loss: 0.9282774
[Epoch 2] mae: 7.595186 val loss: 0.949651
Epochs with no improvement: [ 0 ] and the best   mae  was in  2
[Epoch 3; Iter    24/  488] train: loss: 1.0861803
[Epoch 3; Iter   124/  488] train: loss: 0.3895616
[Epoch 3; Iter   224/  488] train: loss: 0.7670265
[Epoch 3; Iter   324/  488] train: loss: 0.5495901
[Epoch 3; Iter   424/  488] train: loss: 0.6557599
[Epoch 3] mae: 7.598066 val loss: 0.947617
Epochs with no improvement: [ 1 ] and the best   mae  was in  2
[Epoch 4; Iter    36/  488] train: loss: 0.3494554
[Epoch 4; Iter   136/  488] train: loss: 0.8821820
[Epoch 4; Iter   236/  488] train: loss: 0.6322629
[Epoch 4; Iter   336/  488] train: loss: 3.2075624
[Epoch 4; Iter   436/  488] train: loss: 0.5917911
[Epoch 4] mae: 7.582604 val loss: 0.944526
Epochs with no improvement: [ 0 ] and the best   mae  was in  4
[Epoch 5; Iter    48/  488] train: loss: 0.7031323
[Epoch 5; Iter   148/  488] train: loss: 3.7303131
[Epoch 5; Iter   248/  488] train: loss: 0.8750399
[Epoch 5; Iter   348/  488] train: loss: 0.7388062
[Epoch 5; Iter   448/  488] train: loss: 1.0464878
[Epoch 5] mae: 7.576940 val loss: 0.940884
Epochs with no improvement: [ 0 ] and the best   mae  was in  5
[Epoch 6; Iter    60/  488] train: loss: 0.5484984
[Epoch 6; Iter   160/  488] train: loss: 0.7166984
[Epoch 6; Iter   260/  488] train: loss: 0.6712396
[Epoch 6; Iter   360/  488] train: loss: 0.3767663
[Epoch 6; Iter   460/  488] train: loss: 0.6130159
[Epoch 6] mae: 7.515592 val loss: 0.935144
Epochs with no improvement: [ 0 ] and the best   mae  was in  6
[Epoch 7; Iter    72/  488] train: loss: 1.4223821
[Epoch 7; Iter   172/  488] train: loss: 0.4346460
[Epoch 7; Iter   272/  488] train: loss: 0.7531754
[Epoch 7; Iter   372/  488] train: loss: 0.6044096
[Epoch 7; Iter   472/  488] train: loss: 0.6901265
[Epoch 7] mae: 7.553219 val loss: 0.939260
Epochs with no improvement: [ 1 ] and the best   mae  was in  6
[Epoch 8; Iter    84/  488] train: loss: 0.3930788
[Epoch 8; Iter   184/  488] train: loss: 0.6312317
[Epoch 8; Iter   284/  488] train: loss: 0.9061719
[Epoch 8; Iter   384/  488] train: loss: 0.5183470
[Epoch 8; Iter   484/  488] train: loss: 1.0271500
[Epoch 8] mae: 7.509793 val loss: 0.933351
Epochs with no improvement: [ 0 ] and the best   mae  was in  8
[Epoch 9; Iter    96/  488] train: loss: 1.4490143
[Epoch 9; Iter   196/  488] train: loss: 0.3559217
[Epoch 9; Iter   296/  488] train: loss: 0.9178857
[Epoch 9; Iter   396/  488] train: loss: 0.6854767
[Epoch 9] mae: 7.510839 val loss: 0.934422
Epochs with no improvement: [ 1 ] and the best   mae  was in  8
[Epoch 10; Iter     8/  488] train: loss: 1.1862116
[Epoch 10; Iter   108/  488] train: loss: 0.5009042
[Epoch 10; Iter   208/  488] train: loss: 0.3689952
[Epoch 10; Iter   308/  488] train: loss: 1.3991970
[Epoch 10; Iter   408/  488] train: loss: 0.6168097
[Epoch 10] mae: 7.507628 val loss: 0.932498
Epochs with no improvement: [ 0 ] and the best   mae  was in  10
[Epoch 11; Iter    20/  488] train: loss: 0.8911159
[Epoch 11; Iter   120/  488] train: loss: 0.7054629
[Epoch 11; Iter   220/  488] train: loss: 0.7401783
[Epoch 11; Iter   320/  488] train: loss: 0.1479042
[Epoch 11; Iter   420/  488] train: loss: 0.6525337
[Epoch 11] mae: 7.514343 val loss: 0.932400
Epochs with no improvement: [ 1 ] and the best   mae  was in  10
[Epoch 12; Iter    32/  488] train: loss: 1.2895007
[Epoch 12; Iter   132/  488] train: loss: 0.3698097
[Epoch 12; Iter   232/  488] train: loss: 0.1827035
[Epoch 12; Iter   332/  488] train: loss: 0.7462695
[Epoch 12; Iter   432/  488] train: loss: 0.8847470
[Epoch 12] mae: 7.483990 val loss: 0.929220
Epochs with no improvement: [ 0 ] and the best   mae  was in  12
[Epoch 13; Iter    44/  488] train: loss: 1.5540807
[Epoch 13; Iter   144/  488] train: loss: 0.5273554
[Epoch 13; Iter   244/  488] train: loss: 0.7767439
[Epoch 13; Iter   344/  488] train: loss: 0.7432189
[Epoch 13; Iter   444/  488] train: loss: 1.5150366
[Epoch 13] mae: 7.519132 val loss: 0.933327
Epochs with no improvement: [ 1 ] and the best   mae  was in  12
[Epoch 14; Iter    56/  488] train: loss: 1.2662654
[Epoch 14; Iter   156/  488] train: loss: 0.8554630
[Epoch 14; Iter   256/  488] train: loss: 0.3248304
[Epoch 14; Iter   356/  488] train: loss: 1.4857309
[Epoch 14; Iter   456/  488] train: loss: 2.3035252
[Epoch 14] mae: 7.613262 val loss: 0.944453
Epochs with no improvement: [ 2 ] and the best   mae  was in  12
[Epoch 15; Iter    68/  488] train: loss: 1.4217057
[Epoch 15; Iter   168/  488] train: loss: 2.3765888
[Epoch 15; Iter   268/  488] train: loss: 0.3148932
[Epoch 15; Iter   368/  488] train: loss: 1.1565087
[Epoch 15; Iter   468/  488] train: loss: 0.9704725
[Epoch 15] mae: 7.465304 val loss: 0.925640
Epochs with no improvement: [ 0 ] and the best   mae  was in  15
[Epoch 16; Iter    80/  488] train: loss: 0.4284778
[Epoch 16; Iter   180/  488] train: loss: 0.5301138
[Epoch 16; Iter   280/  488] train: loss: 0.5372977
[Epoch 16; Iter   380/  488] train: loss: 1.0001311
[Epoch 16; Iter   480/  488] train: loss: 0.4595715
[Epoch 16] mae: 7.474109 val loss: 0.925767
Epochs with no improvement: [ 1 ] and the best   mae  was in  15
[Epoch 17; Iter    92/  488] train: loss: 0.6728084
[Epoch 17; Iter   192/  488] train: loss: 0.6969371
[Epoch 17; Iter   292/  488] train: loss: 1.1402911
